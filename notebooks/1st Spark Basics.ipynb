{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bcc64ee",
   "metadata": {},
   "source": [
    "# Spark Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d85ad1",
   "metadata": {},
   "source": [
    "This Notebook covers everything you need to run a Spark program and illustrates how a Spark program works:\n",
    "- **SparkContext:** How does Spark *access compute clusters*?\n",
    "- **Resilient Distributed Datasets (RDDs):** How does Spark *distribute data*?\n",
    "- **RDD Operations:** How does Spark *perform compute*?\n",
    "- **RDD Parallelization:** How does Spark *distribute compute*?\n",
    "\n",
    "Finally, we illustrate how you could speed up your program by distributing your computation across nodes in your cluster, e.g. across cpu cores on a local cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8141489",
   "metadata": {},
   "source": [
    "## SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97ca745",
   "metadata": {},
   "source": [
    "The first thing a Spark program must do is to create a `SparkContext` object, which tells Spark how to access a cluster. In this case we are using a `local` cluster by specifying\n",
    "```python\n",
    "master=\"local[4]\"\n",
    "```\n",
    "We use a local cluster with 4 compute nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c04fff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/28 14:57:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/10/28 14:57:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/10/28 14:57:16 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "21/10/28 14:57:16 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"SparkBasics\", master=\"local[4]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b4a3e",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d9150",
   "metadata": {},
   "source": [
    "Spark revolves around the concept of a resilient distributed dataset (RDD).\n",
    "- **resilient**: immutable collection of your data\n",
    "- **distributed**: data can be partitioned across nodes in your cluster\n",
    "\n",
    "An RDD can be created by parallelizing existing collections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e86f9df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_collection = list(range(16))\n",
    "simple_rdd = sc.parallelize(existing_collection)\n",
    "simple_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be67ecff",
   "metadata": {},
   "source": [
    "We can get the data on an RDD by calling `.collect()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40bd2ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bfd29c",
   "metadata": {},
   "source": [
    "We can also collect the data by each compute node by calling `.glom()` and then `.collect()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3913db6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 4) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb7281d",
   "metadata": {},
   "source": [
    "We can see that the **data** on `simple_rdd` is **distributed evenly** across our 4 compute nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6ec7c8",
   "metadata": {},
   "source": [
    "## RDD Operations\n",
    "- **Transformations** are piecewise operations on an RDD, e.g. `map`, `filter`, `groupBy`.\n",
    "  Transformations define a plan of execution this is executed if needed (lazy). Usually the transformation is executed when an action requires the results of a transformation.\n",
    "- **Actions** are operations that require the entire data of an RDD, e.g. `collect`, `reduce`, `count` or `min`, `max` and `variance`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193cbe36",
   "metadata": {},
   "source": [
    "Let's dive into the `map` transformation. `map` requires a hook function which defines the logic of the mapping. \n",
    "\n",
    "Here, our logic is take an item, wait one second, and return the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e94bddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep, time\n",
    "\n",
    "# hook function for the map transformation\n",
    "def wait_one_sec(x):\n",
    "    sleep(1.0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a65dcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.0019 sec to define the transformation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 4.0709 sec to collect the data of temp_rdd.\n",
      "\n",
      "transformed_rdd type: <class 'pyspark.rdd.PipelinedRDD'>\n",
      "result_collect: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] of type <class 'list'>\n",
      "result_sum: 120 of type <class 'int'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define a map() transformation\n",
    "t0 = time()\n",
    "transformed_rdd = simple_rdd.map(wait_one_sec)\n",
    "print(f\"Took {(time()-t0):.4f} sec to define the transformation.\")\n",
    "\n",
    "\n",
    "# Define a collect() action\n",
    "t0 = time()\n",
    "result_collect = transformed_rdd.collect()\n",
    "print(f\"Took {(time()-t0):.4f} sec to collect the data of temp_rdd.\")\n",
    "\n",
    "\n",
    "# Define a sum() action\n",
    "result_sum = simple_rdd.sum()\n",
    "\n",
    "\n",
    "# Results\n",
    "print(f\"\\ntransformed_rdd type: {type(transformed_rdd)}\")\n",
    "print(f\"result_collect: {result_collect} of type {type(result_collect)}\")\n",
    "print(f\"result_sum: {result_sum} of type {type(result_sum)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6087d71a",
   "metadata": {},
   "source": [
    "We see two things:\n",
    "- `wait_one_sec` is scheduled by the `map` transformation but executed by the `collect` action. \n",
    "- Transformations return RDD types.\n",
    "- Actions return actual data, e.g. `list`, `int`, `float` types.\n",
    "\n",
    "Additionally, the work scheduled by the `map` transformation is distributed over our 4 compute nodes. `wait_one_sec` is executed for each item in simple_rdd, i.e. **16 seconds** of waiting. The program finished after roughly **4 seconds**.\n",
    "\n",
    "Let's take a look at **parallelization** with another example and proper compute:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d097415d",
   "metadata": {},
   "source": [
    "## RDD Parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff8c7b0",
   "metadata": {},
   "source": [
    "Lets consider the task of performing some compute with an array of 100,000 random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "470096ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01140603, 0.16433057, 0.25681613, 0.6764169 , 0.67450782])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "random_numbers = np.random.rand(100000)\n",
    "random_numbers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fb6564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distributed and not distributed rdds with random_numbers\n",
    "rdd_not_distribed = sc.parallelize(random_numbers, numSlices=1)\n",
    "rdd = sc.parallelize(random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3602d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import cos\n",
    "\n",
    "def cosine_compute(x):\n",
    "    \"\"\"\n",
    "    This function defines the compute we perform.\n",
    "    \"\"\"\n",
    "    [cos(j*x) for j in range(100)]\n",
    "    return cos(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d81d679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/28 14:57:25 WARN TaskSetManager: Stage 5 contains a task of very large size (1870 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1.4424 sec to execute \"cosine_compute\" on \"rdd\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3.2258 sec to execute \"cosine_compute\" on \"rdd_not_distribed\".\n",
      "Took 3.0310 sec to execute \"cosine_compute\" with python's list comprehension.\n"
     ]
    }
   ],
   "source": [
    "# Distributed rdd\n",
    "t0 = time()\n",
    "rdd.map(cosine_compute).collect()\n",
    "print(f'Took {(time()-t0):.4f} sec to execute \"cosine_compute\" on \"rdd\".')\n",
    "\n",
    "\n",
    "# Not distributed rdd\n",
    "t0 = time()\n",
    "rdd_not_distribed.map(cosine_compute).collect()\n",
    "print(f'Took {(time()-t0):.4f} sec to execute \"cosine_compute\" on \"rdd_not_distribed\".')\n",
    "\n",
    "\n",
    "# Python's native list comprehension\n",
    "t0 = time()\n",
    "[cosine_compute(x) for x in random_numbers]\n",
    "print(f'Took {(time()-t0):.4f} sec to execute \"cosine_compute\" with python\\'s list comprehension.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a433598f",
   "metadata": {},
   "source": [
    "By distributing the compute across our 4 local compute nodes, we **speeded up the computation by roughly 2.5 times**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
