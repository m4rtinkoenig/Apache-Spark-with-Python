{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bcc64ee",
   "metadata": {},
   "source": [
    "# Spark Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e5643",
   "metadata": {},
   "source": [
    "CWD `Apache-Spark-with-Python/notebooks` -> `Apache-Spark-with-Python`\n",
    "\n",
    "(Current Working Directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c150707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.environ.get(\"PWD\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfcfc90",
   "metadata": {},
   "source": [
    "Then import from `src`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a6dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helper.decorators import timing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8141489",
   "metadata": {},
   "source": [
    "## SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97ca745",
   "metadata": {},
   "source": [
    "The first thing a Spark program must do is to create a `SparkContext` object, which tells Spark how to access a cluster. In this case we are using a `local` cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c04fff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/26 20:49:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"SparkBasics\", master=\"local[2]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd21e95b",
   "metadata": {},
   "source": [
    "Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe979ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "url = \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\"\n",
    "destination = \"data/kddcup.data_10_percent.gz\"\n",
    "f = urlretrieve(url, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b4a3e",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d9150",
   "metadata": {},
   "source": [
    "Spark revolves around the concept of a resilient distributed dataset (RDD).\n",
    "- `resilient` immutable collection of your data\n",
    "- `distributed` data can be partitioned across nodes in your cluster\n",
    "\n",
    "1. An RDD can be created by parallelizing existing collections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e86f9df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_collection = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(existing_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f6d6bb",
   "metadata": {},
   "source": [
    "2. An RDD can be created from external datasets, here from local file system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23384ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6ec7c8",
   "metadata": {},
   "source": [
    "## RDD Operations\n",
    "- **transformations** are piecewise operations on an RDD, e.g. `map`, `filter`, `groupBy`.\n",
    "- **actions** are operations that require the entire data of an RDD, e.g. `reduce`, `count` or `min`, `max` and `variance`.\n",
    "\n",
    "More on transformations and actions: https://www.analyticsvidhya.com/blog/2016/10/using-pyspark-to-perform-transformations-and-actions-on-rdd/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e505c",
   "metadata": {},
   "source": [
    "In the following, we will wrap all method calls in `timing`\n",
    "\n",
    "Note: `rdd.some_method(arg)` <=> `timing(rdd.some_method)(arg)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5732c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func: filter(args=(<function <lambda> at 0x108cd7dc0>,), kwargs={}) took: 0.0058 sec\n"
     ]
    }
   ],
   "source": [
    "rdd_filtered = timing(rdd.filter)(lambda x: 'normal.' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffebbee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func: count(args=(), kwargs={}) took: 2.1918 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "97278"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timing(rdd_filtered.count)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10b17af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func: take(args=(5,), kwargs={}) took: 0.0827 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,219,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,39,39,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,217,2032,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,49,49,1.00,0.00,0.02,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timing(rdd_filtered.take)(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f38ae5a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func: map(args=(<function <lambda> at 0x105bd6280>,), kwargs={}) took: 0.0017 sec\n",
      "func: take(args=(5,), kwargs={}) took: 0.0764 sec\n",
      "['0',\n",
      " 'tcp',\n",
      " 'http',\n",
      " 'SF',\n",
      " '181',\n",
      " '5450',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '1',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '8',\n",
      " '8',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '1.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '9',\n",
      " '9',\n",
      " '1.00',\n",
      " '0.00',\n",
      " '0.11',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " 'normal.']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "rdd_split = timing(rdd.map)(lambda x: x.split(\",\"))\n",
    "head_rows = timing(rdd_split.take)(5)\n",
    "pprint(head_rows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e26f736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func: take(args=(100000,), kwargs={}) took: 1.4344 sec\n"
     ]
    }
   ],
   "source": [
    "head_rows = timing(rdd_split.take)(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6841dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_interaction(line):\n",
    "    elems = line.split(\",\")\n",
    "    tag = elems[41]\n",
    "    return (tag, elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8effe17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func: map(args=(<function parse_interaction at 0x105bd6310>,), kwargs={}) took: 0.0010 sec\n",
      "func: take(args=(5,), kwargs={}) took: 0.0814 sec\n",
      "('normal.',\n",
      " ['0',\n",
      "  'tcp',\n",
      "  'http',\n",
      "  'SF',\n",
      "  '181',\n",
      "  '5450',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '1',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '8',\n",
      "  '8',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '1.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '9',\n",
      "  '9',\n",
      "  '1.00',\n",
      "  '0.00',\n",
      "  '0.11',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  'normal.'])\n"
     ]
    }
   ],
   "source": [
    "rdd_tagged = timing(rdd.map)(parse_interaction)\n",
    "head_rows = timing(rdd_tagged.take)(5)\n",
    "pprint(head_rows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9275592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func: collect(args=(), kwargs={}) took: 3.1322 sec\n"
     ]
    }
   ],
   "source": [
    "all_raw_data = timing(rdd.collect)()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb12e2c",
   "metadata": {},
   "source": [
    "# All together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5daed828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func: map(args=(<function parse_interaction at 0x105bd6310>,), kwargs={}) took: 0.0005 sec\n",
      "func: filter(args=(<function <lambda> at 0x1227414c0>,), kwargs={}) took: 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func: collect(args=(), kwargs={}) took: 3.0451 sec\n",
      "There are 97278 'normal' interactions\n"
     ]
    }
   ],
   "source": [
    "# get data from file\n",
    "rdd = sc.textFile(destination)\n",
    "\n",
    "# parse into key-value pairs\n",
    "rdd2 = timing(rdd.map)(parse_interaction)\n",
    "\n",
    "# filter normal key interactions\n",
    "rdd3 = timing(rdd2.filter)(lambda x: x[0] == \"normal.\")\n",
    "\n",
    "# collect all (returns list)\n",
    "all_normal = timing(rdd3.collect)()\n",
    "normal_count = len(all_normal)\n",
    "print(f\"There are {normal_count} 'normal' interactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d81d679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
